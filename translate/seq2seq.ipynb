{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pkuseg\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>读取数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据并处理成中英文列表，两个列表相同索引位置为对应译文\n",
    "def load_data(filename):\n",
    "    en = []\n",
    "    cn = []\n",
    "    seg = pkuseg.pkuseg()\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])\n",
    "            cn.append(['BOS'] + [c for c in line[1]] + ['EOS'])\n",
    "    return en, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和验证集\n",
    "def train_val_split(en, cn, split=0.1):\n",
    "    val_len = int(len(en) * split)\n",
    "    val_index = np.random.choice(len(en), val_len, replace=False)\n",
    "    train_en = []\n",
    "    train_cn = []\n",
    "    val_en = []\n",
    "    val_cn = []\n",
    "    for i in range(len(en)):\n",
    "        if i in val_index:\n",
    "            val_en.append(en[i])\n",
    "            val_cn.append(cn[i])\n",
    "        else:\n",
    "            train_en.append(en[i])\n",
    "            train_cn.append(cn[i])\n",
    "    \n",
    "    return train_en, train_cn, val_en, val_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己找的数据集\n",
    "filename = 'datasets/cmn.txt'\n",
    "en, cn = load_data(filename)\n",
    "train_en, train_cn, val_en, val_cn = train_val_split(en, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_en len: 18120\n",
      "train_cn len: 18120\n",
      "val_en len: 2013\n",
      "val_cn len: 2013\n"
     ]
    }
   ],
   "source": [
    "print('train_en len:', len(train_en))\n",
    "print('train_cn len:', len(train_cn))\n",
    "print('val_en len:', len(val_en))\n",
    "print('val_cn len:', len(val_cn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>构建词表</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "\n",
    "def build_vocab(text, max_words=None):\n",
    "    word_count = Counter()\n",
    "    for sentence in text:\n",
    "        word_count.update(sentence)\n",
    "    if not max_words:\n",
    "        max_words = len(word_count)\n",
    "    word_count = word_count.most_common(max_words)\n",
    "    \n",
    "    vocab_size = max_words + 2\n",
    "    word2ix = {item[0] : i+2 for i, item in enumerate(word_count)}\n",
    "    word2ix['UNK'] = UNK_ID\n",
    "    word2ix['PAD'] = PAD_ID\n",
    "    \n",
    "    return word2ix, vocab_size\n",
    "\n",
    "en_word2ix, en_vocab_size = build_vocab(en)\n",
    "cn_word2ix, cn_vocab_size = build_vocab(cn)\n",
    "\n",
    "en_ix2word = {ix:word for word, ix in en_word2ix.items()}\n",
    "cn_ix2word = {ix:word for word, ix in cn_word2ix.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>构建Dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换成数字编码\n",
    "def encode(en, cn):\n",
    "    encode_en = [[en_word2ix[word] for word in sentence] for sentence in en]\n",
    "    encode_cn = [[cn_word2ix[word] for word in sentence] for sentence in cn]\n",
    "    return encode_en, encode_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字编码后的数据\n",
    "train_data_en, train_data_cn = encode(en,cn)\n",
    "val_data_en, val_data_cn = encode(en, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class translateDataset(Dataset):\n",
    "    def __init__(self, en, cn):\n",
    "        self.en = en\n",
    "        self.cn = cn\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        en_sentence = torch.LongTensor(self.en[index])\n",
    "        cn_sentence = torch.LongTensor(self.cn[index])\n",
    "        return en_sentence, cn_sentence\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    en, cn = zip(*data)\n",
    "    en_lengths = [len(sentence) for sentence in en]\n",
    "    cn_lengths = [len(sentence) for sentence in cn]\n",
    "    \n",
    "    en_batch_length = max(en_lengths)\n",
    "    cn_batch_length = max(cn_lengths)\n",
    "    \n",
    "    target_en = torch.zeros(len(en), en_batch_length).long()\n",
    "    target_cn = torch.zeros(len(cn), cn_batch_length).long()\n",
    "    \n",
    "    for i in range(len(en)):\n",
    "        en_text = en[i]\n",
    "        cn_text = cn[i]\n",
    "        \n",
    "        en_len = en_lengths[i]\n",
    "        cn_len = cn_lengths[i]\n",
    "        \n",
    "        target_en[i, :en_len] = en_text\n",
    "        target_cn[i, :cn_len] = cn_text\n",
    "    en_lengths = torch.LongTensor(en_lengths)\n",
    "    cn_lengths = torch.LongTensor(cn_lengths)\n",
    "    return target_en, en_lengths, target_cn, cn_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size=100, shuffle=True):\n",
    "    train_dataset = translateDataset(train_data_en, train_data_cn)\n",
    "    val_dataset = translateDataset(val_data_en, val_data_cn)\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_dataset\n",
    "                                 ,batch_size=batch_size\n",
    "                                 ,shuffle=shuffle\n",
    "                                 ,pin_memory=True\n",
    "                                 ,collate_fn=collate_fn)\n",
    "    \n",
    "    val_dataloader = DataLoader(dataset=val_dataset\n",
    "                                 ,batch_size=batch_size\n",
    "                                 ,shuffle=shuffle\n",
    "                                 ,pin_memory=True\n",
    "                                 ,collate_fn=collate_fn)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建encoder-decoder模型(没有attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>encoder部分</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        '''\n",
    "        x: (batch_size , seq_len)\n",
    "        lengths: (batch_size,)\n",
    "        '''\n",
    "        sorted_len, sorted_index = lengths.sort(0, descending=True)\n",
    "        \n",
    "        sorted_x = x[sorted_index.long()]\n",
    "        embed = self.dropout(self.embed(sorted_x))\n",
    "        # embed: (batch_size , seq_len , embed_size)\n",
    "        \n",
    "        packed_embed = pack_padded_sequence(embed, sorted_len, batch_first=True)\n",
    "        \n",
    "        out, hidden = self.gru(packed_embed)\n",
    "        # hidden: h_n = (1 , batch_size , hidden_size)\n",
    "        \n",
    "        out, _ = pad_packed_sequence(out, batch_first=True) # 其实encoder部分可以不用out，只需要hidden就行了，但是加了attention就必须要了\n",
    "        # out: (batch_size , seq_len , hidden_size)\n",
    "        \n",
    "        # 下面的操作就是还原index\n",
    "        _, orginal_index = sorted_index.sort(0)\n",
    "        out = out[orginal_index.long()].contiguous()\n",
    "        hidden = hidden[:,orginal_index.long()].contiguous()\n",
    "        # out: (batch_size , seq_len , embed_size)\n",
    "        # hidden: h_n = (1 , batch_size , hidden_size)\n",
    "        \n",
    "        return out, hidden[[-1]]      # [-1]表示取最后一个元素并保持维度数，\n",
    "                                      # 例如hidden的shape为(3,4,5)，那么hidden[[-1]]的维度为(1,4,5)；而hidden[-1]为(4,5)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>decoder部分</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, lengths, hidden):\n",
    "        sorted_len, sorted_index = lengths.sort(0, descending=True)\n",
    "        \n",
    "        sorted_y = y[sorted_index.long()]\n",
    "        hidden = hidden[:, sorted_index.long()]\n",
    "        \n",
    "        embed = self.dropout(self.embed(sorted_y))\n",
    "        \n",
    "        packed_embed = pack_padded_sequence(embed, sorted_len, batch_first=True)\n",
    "        \n",
    "        out, hidden = self.gru(packed_embed, hidden)\n",
    "        # h_n = (1 , batch_size , hidden_size)\n",
    "        out, _ = pad_packed_sequence(out,batch_first=True)\n",
    "        # out: (batch_size , seq_len , embed_size)\n",
    "        \n",
    "        _, orginal_index = sorted_index.sort(0)\n",
    "        \n",
    "        out = out[orginal_index.long()].contiguous()\n",
    "        hidden = hidden[:,orginal_index.long()].contiguous()\n",
    "        \n",
    "        output = self.fc(out)\n",
    "        output = F.log_softmax(output, -1)\n",
    "        # output: (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Seq2seq模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2seq(nn.Module):\n",
    "    def __init__(self, en_vocab_size, cn_vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.2):\n",
    "        super(PlainSeq2seq, self).__init__()\n",
    "        self.encoder = PlainEncoder(en_vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = PlainDecoder(cn_vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "        \n",
    "    def forward(self, en_data, en_lengths, cn_data, cn_lengths):\n",
    "        _, hidden = self.encoder(en_data, en_lengths)\n",
    "        output, _ = self.decoder(cn_data, cn_lengths, hidden)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def translate(self, en, cn_word2ix, cn_ix2word, max_len=10):\n",
    "        '''\n",
    "        输入：一句经过编码的句子,shape为(seq_len, )\n",
    "        '''\n",
    "        en_lengths = torch.LongTensor([len(en)]).to(en.device)\n",
    "        # en_lengths: (1,) , 即batch_size = 1\n",
    "        en = en.unsqueeze(0)\n",
    "        # en: (1, seq_len) , 即batch_size = 1\n",
    "        _, hidden = self.encoder(en, en_lengths)\n",
    "        # hidden: (1, 1, hidden_size)\n",
    "        y = torch.LongTensor([[cn_word2ix['BOS']]]).to(en.device)\n",
    "        # y:(1, 1)\n",
    "        res = []\n",
    "        for i in range(max_len):\n",
    "            output, hidden = self.decoder(y, torch.ones(1,).long().to(y.device),hidden=hidden)\n",
    "            # output: (1,1,vocab_size), 经过log_softmax后的output\n",
    "            y = output.max(2, keepdim=True)[1].view(-1,1)\n",
    "            index = y.item()\n",
    "            res.append(index)\n",
    "            if index==cn_word2ix['EOS']:\n",
    "                break\n",
    "        preds = [cn_ix2word[word] for word in res]\n",
    "        \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>定义损失函数</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, outputs, targets, mask):\n",
    "        '''\n",
    "        outputs: (batch_size, max_seq_len, vocab_size)\n",
    "        targets: (batch_size, max_seq_len)\n",
    "        mask: (batch_size, max_seq_len)\n",
    "        '''\n",
    "        outputs = outputs.contiguous().view(-1, outputs.size(2))\n",
    "        # outputs: (batch_size * max_seq_len,  vocab_size)\n",
    "        targets = targets.contiguous().view(-1, 1)\n",
    "        # targets: (batch_size * max_seq_len, 1)\n",
    "        mask = mask.contiguous().view(-1,1)\n",
    "        # mask: (batch_size * max_seq_len, 1)\n",
    "        \n",
    "        losses = -outputs.gather(1, targets) * mask\n",
    "        \n",
    "        loss = torch.sum(losses) / torch.sum(mask)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>训练函数</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_loader, dev_data_loader, model, optimizer, loss_fn, device, max_epochs=2):\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for iteration, (en_data, en_lengths, cn_data, cn_lengths) in enumerate(train_data_loader):\n",
    "            en_data = en_data.to(device)\n",
    "            en_lengths = en_lengths.to(device)\n",
    "            cn_input = cn_data[:, :-1].to(device)\n",
    "            cn_target = cn_data[:, 1:].to(device)\n",
    "            cn_lengths = (cn_lengths-1).to(device)\n",
    "            \n",
    "            preds = model(en_data, en_lengths, cn_input, cn_lengths)\n",
    "            \n",
    "            mask = torch.arange(cn_lengths.max().item(), device=device)[None,:] < cn_lengths[:,None]\n",
    "            mask = mask.float()\n",
    "            \n",
    "            loss = loss_fn(preds, cn_target, mask)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # 为了防止梯度过大\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration%100 == 0:\n",
    "                print('Epoch: ', epoch, ' |  Iteration', iteration, ' |  loss: ', loss.item())\n",
    "        if epoch % 3 == 0:\n",
    "            dev_loss = evaluate(dev_data_loader, model, loss_fn, device)\n",
    "            if dev_loss < best_loss:\n",
    "                best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>验证函数</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dev_data_loader, model, loss_fn, deivce):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for iteration, (en_data, en_lengths, cn_data, cn_lengths) in enumerate(dev_data_loader):\n",
    "            en_data = en_data.to(device)\n",
    "            en_lengths = en_lengths.to(device)\n",
    "            cn_data = cn_data.to(device)\n",
    "            cn_input = cn_data[:, :-1].to(device)\n",
    "            cn_target = cn_data[:, 1:].to(device)\n",
    "            cn_lengths = (cn_lengths-1).to(device)\n",
    "            \n",
    "            preds = model(en_data, en_lengths, cn_input, cn_lengths)\n",
    "            \n",
    "            mask = torch.arange(cn_lengths.max().item(), device=device)[None,:] < cn_lengths[:,None]\n",
    "            mask = mask.float()\n",
    "            \n",
    "            loss = loss_fn(preds, cn_target, mask)\n",
    "            total_loss += loss.item()\n",
    "    print('Dev Loss: ', total_loss / len(dev_data_loader))\n",
    "    return total_loss / len(dev_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>实例化模型、优化器、损失函数等</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  |  Iteration 0  |  loss:  8.153417587280273\n",
      "Epoch:  0  |  Iteration 100  |  loss:  5.07241153717041\n",
      "Epoch:  0  |  Iteration 200  |  loss:  4.898135185241699\n",
      "Dev Loss:  4.741683891504118\n",
      "Epoch:  1  |  Iteration 0  |  loss:  4.577436923980713\n",
      "Epoch:  1  |  Iteration 100  |  loss:  4.515731334686279\n",
      "Epoch:  1  |  Iteration 200  |  loss:  4.224242210388184\n",
      "Epoch:  2  |  Iteration 0  |  loss:  4.005097389221191\n",
      "Epoch:  2  |  Iteration 100  |  loss:  4.192634105682373\n",
      "Epoch:  2  |  Iteration 200  |  loss:  3.9371724128723145\n",
      "Epoch:  3  |  Iteration 0  |  loss:  3.823545217514038\n",
      "Epoch:  3  |  Iteration 100  |  loss:  3.886296510696411\n",
      "Epoch:  3  |  Iteration 200  |  loss:  3.599013328552246\n",
      "Dev Loss:  3.6594991412493263\n",
      "Epoch:  4  |  Iteration 0  |  loss:  3.8164820671081543\n",
      "Epoch:  4  |  Iteration 100  |  loss:  3.551103115081787\n",
      "Epoch:  4  |  Iteration 200  |  loss:  3.482710123062134\n",
      "Epoch:  5  |  Iteration 0  |  loss:  3.647207021713257\n",
      "Epoch:  5  |  Iteration 100  |  loss:  3.399932622909546\n",
      "Epoch:  5  |  Iteration 200  |  loss:  3.3750739097595215\n",
      "Epoch:  6  |  Iteration 0  |  loss:  3.3543248176574707\n",
      "Epoch:  6  |  Iteration 100  |  loss:  3.465412139892578\n",
      "Epoch:  6  |  Iteration 200  |  loss:  3.273010492324829\n",
      "Dev Loss:  3.237047482245039\n",
      "Epoch:  7  |  Iteration 0  |  loss:  3.3165876865386963\n",
      "Epoch:  7  |  Iteration 100  |  loss:  3.2245192527770996\n",
      "Epoch:  7  |  Iteration 200  |  loss:  3.3680973052978516\n",
      "Epoch:  8  |  Iteration 0  |  loss:  3.2514944076538086\n",
      "Epoch:  8  |  Iteration 100  |  loss:  3.3280081748962402\n",
      "Epoch:  8  |  Iteration 200  |  loss:  3.249937057495117\n",
      "Epoch:  9  |  Iteration 0  |  loss:  3.229794979095459\n",
      "Epoch:  9  |  Iteration 100  |  loss:  2.9603452682495117\n",
      "Epoch:  9  |  Iteration 200  |  loss:  3.2317328453063965\n",
      "Dev Loss:  2.975651486085193\n",
      "Epoch:  10  |  Iteration 0  |  loss:  3.157254695892334\n",
      "Epoch:  10  |  Iteration 100  |  loss:  3.075132131576538\n",
      "Epoch:  10  |  Iteration 200  |  loss:  3.109318494796753\n",
      "Epoch:  11  |  Iteration 0  |  loss:  2.9796855449676514\n",
      "Epoch:  11  |  Iteration 100  |  loss:  3.148280143737793\n",
      "Epoch:  11  |  Iteration 200  |  loss:  3.109311580657959\n",
      "Epoch:  12  |  Iteration 0  |  loss:  2.9184887409210205\n",
      "Epoch:  12  |  Iteration 100  |  loss:  3.0002481937408447\n",
      "Epoch:  12  |  Iteration 200  |  loss:  2.957392930984497\n",
      "Dev Loss:  2.7771097976382415\n",
      "Epoch:  13  |  Iteration 0  |  loss:  2.8574697971343994\n",
      "Epoch:  13  |  Iteration 100  |  loss:  2.9620132446289062\n",
      "Epoch:  13  |  Iteration 200  |  loss:  2.7799370288848877\n",
      "Epoch:  14  |  Iteration 0  |  loss:  2.8896851539611816\n",
      "Epoch:  14  |  Iteration 100  |  loss:  2.9789841175079346\n",
      "Epoch:  14  |  Iteration 200  |  loss:  2.9105136394500732\n",
      "Epoch:  15  |  Iteration 0  |  loss:  2.7933778762817383\n",
      "Epoch:  15  |  Iteration 100  |  loss:  2.9202077388763428\n",
      "Epoch:  15  |  Iteration 200  |  loss:  2.8792166709899902\n",
      "Dev Loss:  2.6223594457796304\n",
      "Epoch:  16  |  Iteration 0  |  loss:  2.6530792713165283\n",
      "Epoch:  16  |  Iteration 100  |  loss:  2.7675039768218994\n",
      "Epoch:  16  |  Iteration 200  |  loss:  2.7974562644958496\n",
      "Epoch:  17  |  Iteration 0  |  loss:  2.899353265762329\n",
      "Epoch:  17  |  Iteration 100  |  loss:  2.7151570320129395\n",
      "Epoch:  17  |  Iteration 200  |  loss:  2.8772239685058594\n",
      "Epoch:  18  |  Iteration 0  |  loss:  2.6738359928131104\n",
      "Epoch:  18  |  Iteration 100  |  loss:  2.770775318145752\n",
      "Epoch:  18  |  Iteration 200  |  loss:  2.752664566040039\n",
      "Dev Loss:  2.4961317718619167\n",
      "Epoch:  19  |  Iteration 0  |  loss:  2.629004955291748\n",
      "Epoch:  19  |  Iteration 100  |  loss:  2.617812395095825\n",
      "Epoch:  19  |  Iteration 200  |  loss:  2.5667965412139893\n",
      "Epoch:  20  |  Iteration 0  |  loss:  2.6956803798675537\n",
      "Epoch:  20  |  Iteration 100  |  loss:  2.5625483989715576\n",
      "Epoch:  20  |  Iteration 200  |  loss:  2.6667487621307373\n",
      "Epoch:  21  |  Iteration 0  |  loss:  2.589094638824463\n",
      "Epoch:  21  |  Iteration 100  |  loss:  2.5960214138031006\n",
      "Epoch:  21  |  Iteration 200  |  loss:  2.6676933765411377\n",
      "Dev Loss:  2.381867453603461\n",
      "Epoch:  22  |  Iteration 0  |  loss:  2.460827112197876\n",
      "Epoch:  22  |  Iteration 100  |  loss:  2.416430950164795\n",
      "Epoch:  22  |  Iteration 200  |  loss:  2.7517638206481934\n",
      "Epoch:  23  |  Iteration 0  |  loss:  2.452540874481201\n",
      "Epoch:  23  |  Iteration 100  |  loss:  2.457040786743164\n",
      "Epoch:  23  |  Iteration 200  |  loss:  2.5745081901550293\n",
      "Epoch:  24  |  Iteration 0  |  loss:  2.4038045406341553\n",
      "Epoch:  24  |  Iteration 100  |  loss:  2.4939591884613037\n",
      "Epoch:  24  |  Iteration 200  |  loss:  2.517052412033081\n",
      "Dev Loss:  2.284835721006488\n",
      "Epoch:  25  |  Iteration 0  |  loss:  2.605647563934326\n",
      "Epoch:  25  |  Iteration 100  |  loss:  2.3914237022399902\n",
      "Epoch:  25  |  Iteration 200  |  loss:  2.433938503265381\n",
      "Epoch:  26  |  Iteration 0  |  loss:  2.4012649059295654\n",
      "Epoch:  26  |  Iteration 100  |  loss:  2.431993007659912\n",
      "Epoch:  26  |  Iteration 200  |  loss:  2.421056032180786\n",
      "Epoch:  27  |  Iteration 0  |  loss:  2.4398550987243652\n",
      "Epoch:  27  |  Iteration 100  |  loss:  2.4873955249786377\n",
      "Epoch:  27  |  Iteration 200  |  loss:  2.3943238258361816\n",
      "Dev Loss:  2.1978126770198934\n",
      "Epoch:  28  |  Iteration 0  |  loss:  2.3375701904296875\n",
      "Epoch:  28  |  Iteration 100  |  loss:  2.5131893157958984\n",
      "Epoch:  28  |  Iteration 200  |  loss:  2.353140115737915\n",
      "Epoch:  29  |  Iteration 0  |  loss:  2.376460075378418\n",
      "Epoch:  29  |  Iteration 100  |  loss:  2.2176051139831543\n",
      "Epoch:  29  |  Iteration 200  |  loss:  2.3373470306396484\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "en_vocab_size = len(en_word2ix)\n",
    "cn_vocab_size = len(cn_word2ix)\n",
    "\n",
    "embed_size = 100\n",
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "lr = 0.01\n",
    "\n",
    "model = PlainSeq2seq(en_vocab_size, cn_vocab_size, embed_size, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = LanguageModelLoss().to(device)\n",
    "\n",
    "best_model = train(train_dataloader, val_dataloader, model, optimizer, loss_fn, device, max_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>实例验证</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev(model, val_en, val_cn, i, device):\n",
    "    en_sent = \" \".join([en_ix2word[idx] for idx in val_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([cn_ix2word[idx] for idx in val_cn[i]])\n",
    "    print(cn_sent)\n",
    "    \n",
    "    input_en = torch.LongTensor(val_en[i]).to(device)\n",
    "    trans = model.translate(input_en, cn_word2ix, cn_ix2word, max_len=10)\n",
    "    if trans[-1] == 'EOS':\n",
    "        trans.pop()\n",
    "    print(\" \".join(trans))\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS try some . EOS\n",
      "BOS 试 试 吧 。 EOS\n",
      "试 试 。\n",
      "-------------------\n",
      "BOS who died ? EOS\n",
      "BOS 谁 死 了 ？ EOS\n",
      "谁 还 是 个 ？\n",
      "-------------------\n",
      "BOS birds fly . EOS\n",
      "BOS 鳥 類 飛 行 。 EOS\n",
      "去 了 醫 院 。\n",
      "-------------------\n",
      "BOS call home ! EOS\n",
      "BOS 打 电 话 回 家 ！ EOS\n",
      "有 人 好 ！\n",
      "-------------------\n",
      "BOS catch him . EOS\n",
      "BOS 抓 住 他 。 EOS\n",
      "抓 住 他 。\n",
      "-------------------\n",
      "BOS come home . EOS\n",
      "BOS 回 家 吧 。 EOS\n",
      "回 来 。\n",
      "-------------------\n",
      "BOS do it now . EOS\n",
      "BOS 現 在 就 做 。 EOS\n",
      "現 在 正 在 吃 。\n",
      "-------------------\n",
      "BOS dogs bark . EOS\n",
      "BOS 狗 会 叫 。 EOS\n",
      "猫 吧 ！\n",
      "-------------------\n",
      "BOS do n't cry . EOS\n",
      "BOS 别 哭 。 EOS\n",
      "别 人 了 。\n",
      "-------------------\n",
      "BOS excuse me . EOS\n",
      "BOS 对 不 起 。 EOS\n",
      "我 不 要 打 網 球 。\n",
      "-------------------\n",
      "BOS feel this . EOS\n",
      "BOS 来 感 受 一 下 这 个 。 EOS\n",
      "這 可 能 。\n",
      "-------------------\n",
      "BOS follow me . EOS\n",
      "BOS 请 跟 我 来 。 EOS\n",
      "請 給 我 。\n",
      "-------------------\n",
      "BOS follow us . EOS\n",
      "BOS 请 跟 着 我 们 。 EOS\n",
      "别 人 来 说 了 。\n",
      "-------------------\n",
      "BOS good luck . EOS\n",
      "BOS 祝 你 好 运 。 EOS\n",
      "祝 贺 你 。\n",
      "-------------------\n",
      "BOS grab that . EOS\n",
      "BOS 抓 住 那 个 。 EOS\n",
      "繼 續 工 作 。\n",
      "-------------------\n",
      "BOS grab this . EOS\n",
      "BOS 抓 住 这 个 。 EOS\n",
      "請 這 個 工 作 。\n",
      "-------------------\n",
      "BOS hands off . EOS\n",
      "BOS 手 举 起 来 。 EOS\n",
      "上 班 。\n",
      "-------------------\n",
      "BOS he 's a dj . EOS\n",
      "BOS 他 是 一 个   D J   。 EOS\n",
      "他 是 一 個 好 人 。\n",
      "-------------------\n",
      "BOS he 's lazy . EOS\n",
      "BOS 他 很 懒 。 EOS\n",
      "他 是 個 好 人 。\n",
      "-------------------\n",
      "BOS hold fire . EOS\n",
      "BOS 停 火 。 EOS\n",
      "上 帝 。\n",
      "-------------------\n",
      "BOS hold this . EOS\n",
      "BOS 我 住 这 个 。 EOS\n",
      "这 个 很 好 。\n",
      "-------------------\n",
      "BOS how awful ! EOS\n",
      "BOS 太 可 怕 了 。 EOS\n",
      "它 多 久 ！\n",
      "-------------------\n",
      "BOS i am cold . EOS\n",
      "BOS 我 冷 。 EOS\n",
      "我 很 喜 歡 。\n",
      "-------------------\n",
      "BOS i am okay . EOS\n",
      "BOS 我 沒 事 。 EOS\n",
      "我 真 是 很 好 。\n",
      "-------------------\n",
      "BOS i am sick . EOS\n",
      "BOS 我 生 病 了 。 EOS\n",
      "我 很 高 興 。\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,125):\n",
    "    translate_dev(best_model, val_data_en, val_data_cn, i, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建encoder-decoder模型(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**实现注意事项**</font>  \n",
    "1. Luong Attetion模型和Bahdanau Attention模型的不同  \n",
    "2. 注意训练过程rnn的输出并不需要输入进下一个time step中，而是仅仅在预测阶段才输入下一个time step中。这其实就是<font color='red'>**teacher forcing**</font>  \n",
    "3. 在rnn的输出h_n中,h_n[0]表示第一层的前向输出的隐藏状态, h_n[1]表示第一层的后向输出的隐藏状态；h_n[2]表示第二层...前向...,h_n[3]...第二层...后向..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>encoder部分</font>  \n",
    "1. 双向GRU  \n",
    "2. 两层堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, en_hidden_size, de_hidden_size, num_layers=2, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(2 * en_hidden_size, de_hidden_size)\n",
    "        \n",
    "    def forward(self, en, en_lengths):\n",
    "        # en : (batch_size, max_seq_len)\n",
    "        # en_lengths : (batch_size, )\n",
    "        sorted_len, sorted_idx = en_lengths.sort(0, descending=True)\n",
    "        \n",
    "        sorted_en = en[sorted_idx.long()]\n",
    "        \n",
    "        embed = self.dropout(self.embed(sorted_en))\n",
    "        # embed : (batch_size, max_seq_len, embed_size)\n",
    "        \n",
    "        packed_embed = pack_padded_sequence(embed, sorted_len, batch_first=True)\n",
    "        \n",
    "        packed_output, hidden = self.gru(packed_embed)\n",
    "        \n",
    "        # hidden : (num_layers * num_directions, batch_size, en_hidden_size)\n",
    "        #          (2, batch_size, en_hidden_size) (由于gru的bidirectional为True，所以这里有个2)\n",
    "        \n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # output: (batch_size, seq_len, 2 * hidden_size) (由于gru的bidirectional为True，所以这里有个2)\n",
    "        # 这个output在attention中需要\n",
    "        _, orginal_idx = sorted_idx.sort(0)\n",
    "        output = output[orginal_idx.long()].contiguous()\n",
    "        \n",
    "        ##############################\n",
    "        # 错误代码： hidden = hidden[orginal_idx.long()].contiguous()\n",
    "        # 这句代码卡了一个晚上加一个白天，居然是维度弄错了。。。真是醉了，用的测试用例因为数量太少居然通过了\n",
    "        \n",
    "        hidden = hidden[:, orginal_idx.long()].contiguous()\n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 由于在Luong Attention模型中decoder为单向的RNN，所以encoder输出的hidden要作一些变换\n",
    "        hidden = torch.cat((hidden[0::2], hidden[1::2]), dim=2)\n",
    "        # hidden: (num_layers, batch_size, en_hidden_size * 2)\n",
    "        hidden = self.fc(hidden)\n",
    "        # hidden: (num_layers, batch_size, de_hidden_size)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Attention部分</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**实现注意事项**</font>  \n",
    "1. 计算context的时候，需要将encoder的output的隐状态转换为与decoder隐状态相同的维度(这是fc_in为（en_hidden_size * 2, de_hidden_size）的原因)，但是在计算attention后的向量的时候用的还是原来的encoder的output(这是为什么fc\\_out为（en_hidden_size * 2 + de_hidden_size, de_hidden_size）的原因)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, en_hidden_size, de_hidden_size):\n",
    "        # 由于encoder是bidirectional，所以先要处理encoder的output\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc_in = nn.Linear(en_hidden_size * 2, de_hidden_size)\n",
    "        self.fc_out = nn.Linear(en_hidden_size * 2 + de_hidden_size, de_hidden_size)\n",
    "    \n",
    "    def forward(self, ht, en_output, mask):\n",
    "        # ht: (batch_size, de_seq_len, de_hidden_size)\n",
    "        # en_output: (batch_size, en_seq_len, 2 * en_hidden_size)\n",
    "        \n",
    "        hs = self.fc_in(en_output)\n",
    "        # hs: (batch_size, en_seq_len, de_hidden_size)\n",
    "        \n",
    "        score = ht.bmm(hs.transpose(1,2))\n",
    "        # socre:(batch_size, de_seq_len, en_seq_len)\n",
    "        # score[i,j] 表示decoder的第i个单词隐藏状态的输出和encoder第j个单词隐藏状态输出的得分\n",
    "        \n",
    "        score.data.masked_fill(mask, 1e-6) # 主要是为了防止0经过softmax后占掉一部分比例，使其它有效的区域占的比例下降\n",
    "        \n",
    "        attn = F.softmax(score, dim=2)\n",
    "        # attn:(batch_size, de_seq_len, en_seq_len)\n",
    "        \n",
    "        context = torch.bmm(attn, en_output)\n",
    "        # context: (batch_size, de_seq_len, 2*en_hidden_size)\n",
    "        ht_hat = self.fc_out(torch.cat((context, ht), dim=2))\n",
    "        ht_hat = torch.tanh(ht_hat)\n",
    "        # ht_hat: (batch_size, de_seq_len, de_hidden_size)\n",
    "        \n",
    "        return ht_hat, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Decoder部分</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 单向GRU(与encoder不同)  \n",
    "2. 两层堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoungAttnDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, en_hidden_size, de_hidden_size, num_layers=2, dropout=0.2):\n",
    "        super(LoungAttnDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, de_hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.attention = Attention(en_hidden_size, de_hidden_size)\n",
    "        self.fc = nn.Linear(de_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def create_mask(self, en_lengths, cn_lengths, device):\n",
    "        en_max_len = en_lengths.max()\n",
    "        cn_max_len = cn_lengths.max()\n",
    "        \n",
    "        mask_en = torch.arange(en_max_len, device=device)[None,:] < en_lengths[:, None]\n",
    "        # mask_en: (batch_size, en_seq_len)\n",
    "        mask_cn = torch.arange(cn_max_len, device=device)[None,:] < cn_lengths[:, None]\n",
    "        # mask_cn: (batch_size, cn_seq_len)\n",
    "        mask = (mask_en[:,None,:] * mask_cn[:,:,None]).logical_not()\n",
    "        # mask : (batch_size, cn_seq_len, en_seq_len)\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    def forward(self, en_output,en_lengths, cn, cn_lengths, hidden):\n",
    "        sorted_len, sorted_idx = cn_lengths.sort(0, descending=True)\n",
    "        \n",
    "        sorted_cn = cn[sorted_idx.long()]\n",
    "        hidden = hidden[:, sorted_idx.long()]\n",
    "        \n",
    "        embed = self.dropout(self.embed(sorted_cn))\n",
    "        packed_embed = pack_padded_sequence(embed, sorted_len, batch_first=True)\n",
    "        \n",
    "        packed_out, hidden = self.gru(packed_embed, hidden)\n",
    "        # hidden: (num_layers * num_directions, batch_size, de_hidden_size)    这个hidden似乎没用到\n",
    "        #         (2, batch_size, de_hidden_size) \n",
    "        \n",
    "        output, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        # output : (batch_size, max_seq_len, de_hidden_size)\n",
    "        _, original_idx = sorted_idx.sort(0)\n",
    "        \n",
    "        output = output[original_idx.long()].contiguous()\n",
    "        hidden = hidden[:, original_idx.long()].contiguous()\n",
    "        # output : (batch_size, max_seq_len, de_hidden_size)\n",
    "        # hidden: (num_layers * num_directions, batch_size, de_hidden_size)\n",
    "        \n",
    "        # 由于padding的存在，不是所有的句子的max_seq_len长度都要用到，所以此时需要用一个mask来屏蔽padding的影响。\n",
    "        mask = self.create_mask(en_lengths, cn_lengths, cn.device)\n",
    "        \n",
    "        ht_hat, attn = self.attention(output, en_output, mask)\n",
    "        # ht_hat: (batch_size, de_seq_len, de_hidden_size)\n",
    "        # attn:(batch_size, de_seq_len, en_seq_len)\n",
    "        \n",
    "        output = F.log_softmax(self.fc(ht_hat), -1)\n",
    "        \n",
    "        return output, hidden, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Seq2seq部分</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, en_vocab_size, cn_vocab_size, embed_size, en_hidden_size,de_hidden_size, num_layers=2, dropout=0.2):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.encoder = Encoder(en_vocab_size, embed_size, en_hidden_size, de_hidden_size, num_layers, dropout)\n",
    "        self.decoder = LoungAttnDecoder(cn_vocab_size, embed_size, en_hidden_size, de_hidden_size, num_layers, dropout)\n",
    "        \n",
    "    def forward(self, en_data, en_lengths, cn_data, cn_lengths):\n",
    "        en_output, hidden = self.encoder(en_data, en_lengths)\n",
    "        output, _, attn = self.decoder(en_output, en_lengths, cn_data, cn_lengths, hidden)\n",
    "        \n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, en, cn_word2ix, cn_ix2word, max_len=100):\n",
    "        '''\n",
    "        输入：一句经过编码的句子,shape为(seq_len, )\n",
    "        '''\n",
    "        en_lengths = torch.LongTensor([len(en)]).to(en.device)\n",
    "        # en_lengths: (1,) , 即batch_size = 1\n",
    "        en = en.unsqueeze(0)\n",
    "        # en: (1, seq_len) , 即batch_size = 1\n",
    "        en_output, hidden = self.encoder(en, en_lengths)\n",
    "        # hidden: (1, 1, hidden_size)\n",
    "        y = torch.LongTensor([[cn_word2ix['BOS']]]).to(en.device)\n",
    "        # y:(1, 1)\n",
    "        res = []\n",
    "        for i in range(max_len):\n",
    "            output, hidden, _ = self.decoder(en_output, en_lengths, y, torch.ones(1,).long().to(y.device),hidden=hidden)\n",
    "            # output: (1,1,vocab_size), 经过log_softmax后的output\n",
    "            y = output.max(2, keepdim=True)[1].view(-1,1)\n",
    "            index = y.item()\n",
    "            res.append(index)\n",
    "            if index==cn_word2ix['EOS']:\n",
    "                break\n",
    "        preds = [cn_ix2word[word] for word in res]\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>训练函数</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data_loader, dev_data_loader, model, optimizer, loss_fn, device, max_epochs=2):\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for iteration, (en_data, en_lengths, cn_data, cn_lengths) in enumerate(train_data_loader):\n",
    "            en_data = en_data.to(device)\n",
    "            en_lengths = en_lengths.to(device)\n",
    "            cn_input = cn_data[:, :-1].to(device)\n",
    "            cn_target = cn_data[:, 1:].to(device)\n",
    "            cn_lengths = (cn_lengths-1).to(device)\n",
    "            \n",
    "            preds, attn = model(en_data, en_lengths, cn_input, cn_lengths)\n",
    "            \n",
    "            mask = torch.arange(cn_lengths.max().item(), device=device)[None,:] < cn_lengths[:,None]\n",
    "            mask = mask.float()\n",
    "            \n",
    "            loss = loss_fn(preds, cn_target, mask)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # 为了防止梯度过大\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration%100 == 0:\n",
    "                print('Epoch: ', epoch, ' |  Iteration', iteration, ' |  loss: ', loss.item())\n",
    "        if epoch % 3 == 0:\n",
    "            dev_loss = evaluate(dev_data_loader, model, loss_fn, device)\n",
    "            if dev_loss < best_loss:\n",
    "                best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>验证函数</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dev_data_loader, model, loss_fn, deivce):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for iteration, (en_data, en_lengths, cn_data, cn_lengths) in enumerate(dev_data_loader):\n",
    "            en_data = en_data.to(device)\n",
    "            en_lengths = en_lengths.to(device)\n",
    "            cn_data = cn_data.to(device)\n",
    "            cn_input = cn_data[:, :-1].to(device)\n",
    "            cn_target = cn_data[:, 1:].to(device)\n",
    "            cn_lengths = (cn_lengths-1).to(device)\n",
    "            \n",
    "            preds, attn = model(en_data, en_lengths, cn_input, cn_lengths)\n",
    "            \n",
    "            mask = torch.arange(cn_lengths.max().item(), device=device)[None,:] < cn_lengths[:,None]\n",
    "            mask = mask.float()\n",
    "            \n",
    "            loss = loss_fn(preds, cn_target, mask)\n",
    "            total_loss += loss.item()\n",
    "    print('Dev Loss: ', total_loss / len(dev_data_loader))\n",
    "    return total_loss / len(dev_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, outputs, targets, mask):\n",
    "        '''\n",
    "        outputs: (batch_size, max_seq_len, vocab_size)\n",
    "        targets: (batch_size, max_seq_len)\n",
    "        mask: (batch_size, max_seq_len)\n",
    "        '''\n",
    "        outputs = outputs.contiguous().view(-1, outputs.size(2))\n",
    "        # outputs: (batch_size * max_seq_len,  vocab_size)\n",
    "        targets = targets.contiguous().view(-1, 1)\n",
    "        # targets: (batch_size * max_seq_len, 1)\n",
    "        mask = mask.contiguous().view(-1,1)\n",
    "        # mask: (batch_size * max_seq_len, 1)\n",
    "        \n",
    "        losses = -outputs.gather(1, targets) * mask\n",
    "        \n",
    "        loss = torch.sum(losses) / torch.sum(mask)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>实例化模型、优化器、损失函数等</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  |  Iteration 0  |  loss:  8.137531280517578\n",
      "Epoch:  0  |  Iteration 100  |  loss:  5.218105792999268\n",
      "Epoch:  0  |  Iteration 200  |  loss:  5.214974880218506\n",
      "Dev Loss:  5.083258926278294\n",
      "Epoch:  1  |  Iteration 0  |  loss:  5.079915523529053\n",
      "Epoch:  1  |  Iteration 100  |  loss:  4.762298107147217\n",
      "Epoch:  1  |  Iteration 200  |  loss:  4.696444034576416\n",
      "Epoch:  2  |  Iteration 0  |  loss:  4.776860237121582\n",
      "Epoch:  2  |  Iteration 100  |  loss:  4.378644943237305\n",
      "Epoch:  2  |  Iteration 200  |  loss:  4.4215779304504395\n",
      "Epoch:  3  |  Iteration 0  |  loss:  4.393805503845215\n",
      "Epoch:  3  |  Iteration 100  |  loss:  4.339521408081055\n",
      "Epoch:  3  |  Iteration 200  |  loss:  4.180355072021484\n",
      "Dev Loss:  4.011054868745331\n",
      "Epoch:  4  |  Iteration 0  |  loss:  4.12247896194458\n",
      "Epoch:  4  |  Iteration 100  |  loss:  3.9052987098693848\n",
      "Epoch:  4  |  Iteration 200  |  loss:  3.684964656829834\n",
      "Epoch:  5  |  Iteration 0  |  loss:  3.602047920227051\n",
      "Epoch:  5  |  Iteration 100  |  loss:  3.773482084274292\n",
      "Epoch:  5  |  Iteration 200  |  loss:  3.541755199432373\n",
      "Epoch:  6  |  Iteration 0  |  loss:  3.6337854862213135\n",
      "Epoch:  6  |  Iteration 100  |  loss:  3.4503886699676514\n",
      "Epoch:  6  |  Iteration 200  |  loss:  3.6849253177642822\n",
      "Dev Loss:  3.2822702119846157\n",
      "Epoch:  7  |  Iteration 0  |  loss:  3.1647415161132812\n",
      "Epoch:  7  |  Iteration 100  |  loss:  3.493962049484253\n",
      "Epoch:  7  |  Iteration 200  |  loss:  3.2603375911712646\n",
      "Epoch:  8  |  Iteration 0  |  loss:  3.190708875656128\n",
      "Epoch:  8  |  Iteration 100  |  loss:  3.2208988666534424\n",
      "Epoch:  8  |  Iteration 200  |  loss:  3.1304311752319336\n",
      "Epoch:  9  |  Iteration 0  |  loss:  2.9685909748077393\n",
      "Epoch:  9  |  Iteration 100  |  loss:  3.016206979751587\n",
      "Epoch:  9  |  Iteration 200  |  loss:  2.992511510848999\n",
      "Dev Loss:  2.759713813810065\n",
      "Epoch:  10  |  Iteration 0  |  loss:  2.968048572540283\n",
      "Epoch:  10  |  Iteration 100  |  loss:  2.792451858520508\n",
      "Epoch:  10  |  Iteration 200  |  loss:  2.8674471378326416\n",
      "Epoch:  11  |  Iteration 0  |  loss:  2.6603264808654785\n",
      "Epoch:  11  |  Iteration 100  |  loss:  2.7207419872283936\n",
      "Epoch:  11  |  Iteration 200  |  loss:  2.6497750282287598\n",
      "Epoch:  12  |  Iteration 0  |  loss:  2.4414167404174805\n",
      "Epoch:  12  |  Iteration 100  |  loss:  2.7485029697418213\n",
      "Epoch:  12  |  Iteration 200  |  loss:  2.6610422134399414\n",
      "Dev Loss:  2.3558143979251973\n",
      "Epoch:  13  |  Iteration 0  |  loss:  2.609168529510498\n",
      "Epoch:  13  |  Iteration 100  |  loss:  2.5832009315490723\n",
      "Epoch:  13  |  Iteration 200  |  loss:  2.535489082336426\n",
      "Epoch:  14  |  Iteration 0  |  loss:  2.3694369792938232\n",
      "Epoch:  14  |  Iteration 100  |  loss:  2.465545654296875\n",
      "Epoch:  14  |  Iteration 200  |  loss:  2.354606866836548\n",
      "Epoch:  15  |  Iteration 0  |  loss:  2.455272674560547\n",
      "Epoch:  15  |  Iteration 100  |  loss:  2.4172189235687256\n",
      "Epoch:  15  |  Iteration 200  |  loss:  2.391273021697998\n",
      "Dev Loss:  2.0469196527311118\n",
      "Epoch:  16  |  Iteration 0  |  loss:  2.1467838287353516\n",
      "Epoch:  16  |  Iteration 100  |  loss:  2.2139155864715576\n",
      "Epoch:  16  |  Iteration 200  |  loss:  2.2120232582092285\n",
      "Epoch:  17  |  Iteration 0  |  loss:  1.9505198001861572\n",
      "Epoch:  17  |  Iteration 100  |  loss:  2.280132293701172\n",
      "Epoch:  17  |  Iteration 200  |  loss:  2.235151529312134\n",
      "Epoch:  18  |  Iteration 0  |  loss:  2.168520212173462\n",
      "Epoch:  18  |  Iteration 100  |  loss:  2.0638785362243652\n",
      "Epoch:  18  |  Iteration 200  |  loss:  2.3861067295074463\n",
      "Dev Loss:  1.795004921974522\n",
      "Epoch:  19  |  Iteration 0  |  loss:  2.2320752143859863\n",
      "Epoch:  19  |  Iteration 100  |  loss:  2.160820484161377\n",
      "Epoch:  19  |  Iteration 200  |  loss:  2.1913578510284424\n",
      "Epoch:  20  |  Iteration 0  |  loss:  1.900689959526062\n",
      "Epoch:  20  |  Iteration 100  |  loss:  2.187954902648926\n",
      "Epoch:  20  |  Iteration 200  |  loss:  2.028618335723877\n",
      "Epoch:  21  |  Iteration 0  |  loss:  1.9644125699996948\n",
      "Epoch:  21  |  Iteration 100  |  loss:  2.0696494579315186\n",
      "Epoch:  21  |  Iteration 200  |  loss:  1.7974941730499268\n",
      "Dev Loss:  1.5959735129139212\n",
      "Epoch:  22  |  Iteration 0  |  loss:  1.7265212535858154\n",
      "Epoch:  22  |  Iteration 100  |  loss:  1.9175540208816528\n",
      "Epoch:  22  |  Iteration 200  |  loss:  1.7908343076705933\n",
      "Epoch:  23  |  Iteration 0  |  loss:  1.7876579761505127\n",
      "Epoch:  23  |  Iteration 100  |  loss:  1.8457757234573364\n",
      "Epoch:  23  |  Iteration 200  |  loss:  1.8890955448150635\n",
      "Epoch:  24  |  Iteration 0  |  loss:  1.950708031654358\n",
      "Epoch:  24  |  Iteration 100  |  loss:  1.6315839290618896\n",
      "Epoch:  24  |  Iteration 200  |  loss:  1.9717917442321777\n",
      "Dev Loss:  1.4081117605218794\n",
      "Epoch:  25  |  Iteration 0  |  loss:  1.6906582117080688\n",
      "Epoch:  25  |  Iteration 100  |  loss:  1.909725308418274\n",
      "Epoch:  25  |  Iteration 200  |  loss:  1.547951579093933\n",
      "Epoch:  26  |  Iteration 0  |  loss:  1.68600332736969\n",
      "Epoch:  26  |  Iteration 100  |  loss:  1.833135724067688\n",
      "Epoch:  26  |  Iteration 200  |  loss:  1.5409470796585083\n",
      "Epoch:  27  |  Iteration 0  |  loss:  1.6735386848449707\n",
      "Epoch:  27  |  Iteration 100  |  loss:  1.59834623336792\n",
      "Epoch:  27  |  Iteration 200  |  loss:  1.7034742832183838\n",
      "Dev Loss:  1.2680320084685146\n",
      "Epoch:  28  |  Iteration 0  |  loss:  1.5228573083877563\n",
      "Epoch:  28  |  Iteration 100  |  loss:  1.6264190673828125\n",
      "Epoch:  28  |  Iteration 200  |  loss:  1.6726595163345337\n",
      "Epoch:  29  |  Iteration 0  |  loss:  1.511447548866272\n",
      "Epoch:  29  |  Iteration 100  |  loss:  1.6232457160949707\n",
      "Epoch:  29  |  Iteration 200  |  loss:  1.6028318405151367\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "en_vocab_size = len(en_word2ix)\n",
    "cn_vocab_size = len(cn_word2ix)\n",
    "\n",
    "embed_size = 100\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "lr = 0.01\n",
    "\n",
    "model = Seq2seq(en_vocab_size, cn_vocab_size, embed_size, hidden_size, hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = LanguageModelLoss().to(device)\n",
    "\n",
    "best_model = train(train_dataloader, val_dataloader, model, optimizer, loss_fn, device, max_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>实例验证</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev(model, val_en, val_cn, i, device):\n",
    "    en_sent = \" \".join([en_ix2word[idx] for idx in val_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([cn_ix2word[idx] for idx in val_cn[i]])\n",
    "    print(cn_sent)\n",
    "    \n",
    "    input_en = torch.LongTensor(val_en[i]).to(device)\n",
    "    trans = model.translate(input_en, cn_word2ix, cn_ix2word, max_len=10)\n",
    "    if trans[-1] == 'EOS':\n",
    "        trans.pop()\n",
    "    print(\" \".join(trans))\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>效果明显比不带attention的好多了</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS try some . EOS\n",
      "BOS 试 试 吧 。 EOS\n",
      "试 试 了 。\n",
      "-------------------\n",
      "BOS who died ? EOS\n",
      "BOS 谁 死 了 ？ EOS\n",
      "谁 死 了 ？\n",
      "-------------------\n",
      "BOS birds fly . EOS\n",
      "BOS 鳥 類 飛 行 。 EOS\n",
      "鳥 鳥 類 動 了 。\n",
      "-------------------\n",
      "BOS call home ! EOS\n",
      "BOS 打 电 话 回 家 ！ EOS\n",
      "叫 回 家 ！\n",
      "-------------------\n",
      "BOS catch him . EOS\n",
      "BOS 抓 住 他 。 EOS\n",
      "抓 住 了 。\n",
      "-------------------\n",
      "BOS come home . EOS\n",
      "BOS 回 家 吧 。 EOS\n",
      "回 家 。\n",
      "-------------------\n",
      "BOS do it now . EOS\n",
      "BOS 現 在 就 做 。 EOS\n",
      "现 在 做 。\n",
      "-------------------\n",
      "BOS dogs bark . EOS\n",
      "BOS 狗 会 叫 。 EOS\n",
      "狗 叫 。\n",
      "-------------------\n",
      "BOS do n't cry . EOS\n",
      "BOS 别 哭 。 EOS\n",
      "别 哭 。\n",
      "-------------------\n",
      "BOS excuse me . EOS\n",
      "BOS 对 不 起 。 EOS\n",
      "对 我 来 。\n",
      "-------------------\n",
      "BOS feel this . EOS\n",
      "BOS 来 感 受 一 下 这 个 。 EOS\n",
      "来 感 到 这 个 。\n",
      "-------------------\n",
      "BOS follow me . EOS\n",
      "BOS 请 跟 我 来 。 EOS\n",
      "请 帮 我 。\n",
      "-------------------\n",
      "BOS follow us . EOS\n",
      "BOS 请 跟 着 我 们 。 EOS\n",
      "我 們 想 要 我 們 的 。\n",
      "-------------------\n",
      "BOS good luck . EOS\n",
      "BOS 祝 你 好 运 。 EOS\n",
      "祝 你 好 。\n",
      "-------------------\n",
      "BOS grab that . EOS\n",
      "BOS 抓 住 那 个 。 EOS\n",
      "抓 住 那 个 。\n",
      "-------------------\n",
      "BOS grab this . EOS\n",
      "BOS 抓 住 这 个 。 EOS\n",
      "抓 住 这 个 。\n",
      "-------------------\n",
      "BOS hands off . EOS\n",
      "BOS 手 举 起 来 。 EOS\n",
      "水 吸 烟 了 。\n",
      "-------------------\n",
      "BOS he 's a dj . EOS\n",
      "BOS 他 是 一 个   D J   。 EOS\n",
      "他 是 一 個 等 。\n",
      "-------------------\n",
      "BOS he 's lazy . EOS\n",
      "BOS 他 很 懒 。 EOS\n",
      "他 很 高 。\n",
      "-------------------\n",
      "BOS hold fire . EOS\n",
      "BOS 停 火 。 EOS\n",
      "停 火 。\n",
      "-------------------\n",
      "BOS hold this . EOS\n",
      "BOS 我 住 这 个 。 EOS\n",
      "把 这 个 。\n",
      "-------------------\n",
      "BOS how awful ! EOS\n",
      "BOS 太 可 怕 了 。 EOS\n",
      "真 的 需 要 。\n",
      "-------------------\n",
      "BOS i am cold . EOS\n",
      "BOS 我 冷 。 EOS\n",
      "我 很 冷 。\n",
      "-------------------\n",
      "BOS i am okay . EOS\n",
      "BOS 我 沒 事 。 EOS\n",
      "我 會 進 了 。\n",
      "-------------------\n",
      "BOS i am sick . EOS\n",
      "BOS 我 生 病 了 。 EOS\n",
      "我 生 病 了 。\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,125):\n",
    "    translate_dev(best_model, val_data_en, val_data_cn, i, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
